{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b621784",
   "metadata": {},
   "source": [
    "### Relevant Links\n",
    "- https://openai.com/blog/introducing-text-and-code-embeddings/\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\n",
    "- https://platform.openai.com/docs/api-reference/completions/create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a152e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from importlib import reload\n",
    "\n",
    "with open(\"openai_api.json\") as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "openai.api_version = creds[\"api_version\"]\n",
    "openai.api_base = creds[\"api_base\"]\n",
    "openai.api_type = creds[\"api_type\"]\n",
    "openai.api_key = creds[\"api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e63350ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models:\n",
    "# [(m.id, m.status) for m in openai.Model.list()[\"data\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442675d",
   "metadata": {},
   "source": [
    "### Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f05a8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(p_context, p_question):\n",
    "    p_prompt = f\"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{p_context}\n",
    "Question: {p_question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return p_prompt\n",
    "\n",
    "def complete(p_prompt):\n",
    "    response = openai.Completion.create(\n",
    "        prompt=p_prompt,\n",
    "        deployment_id=\"text-chat-davinci-002\",  # token limit: ~4000\n",
    "        temperature=0.0,\n",
    "        max_tokens=150,\n",
    "        stop=\"\\n\\n\",\n",
    "    )\n",
    "    # (no. of tokens in prompt + max_tokens arg) should be < token limit\n",
    "    # return response\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7e725af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florence-VL, as part of Project Florence, is about building new foundation models for Multimodal Intelligence. The goal is to endow computers with an ability to effectively learn from multi-modality (or multi-channel) data, similar to sights and sounds attained from vision and language that help humans make sense of the world around us. The project is funded by the Microsoft AI Cognitive Service team since 2020 and aims to advance the state of the art on vision-language modeling and develop the best computer vision technologies as part of the mission to empower everyone on the planet to achieve more. \n"
     ]
    }
   ],
   "source": [
    "# context taken from: https://www.microsoft.com/en-us/research/project/project-florence-vl/\n",
    "context = \"\"\"One of the core aspirations in artificial intelligence is to develop\n",
    "algorithms that endow computers with an ability to effectively learn from\n",
    "multi-modality (or multi-channel) data. This data is similar to sights and\n",
    "sounds attained from vision and language that help humans make sense of the\n",
    "world around us. For example, computers could mimic this ability by searching\n",
    "the most similar images for a text query (or vice versa) and by describing the\n",
    "content of an image using natural language.\n",
    "Azure Florence-Vision and Language, short for Florence-VL, is launched to\n",
    "achieve this goal, where we aim to build new foundation models for Multimodal\n",
    "Intelligence. Florence-VL, as part of Project\n",
    "Florence, is funded by the Microsoft AI\n",
    "Cognitive Service team since 2020. Motivated by the strong demand from real\n",
    "applications and recent research progresses on computer vision, natural\n",
    "language processing, and vision-language understanding, we strive to advance\n",
    "the state of the art on vision-language modeling and develop the best computer\n",
    "vision technologies as part of our mission to empower everyone on the planet\n",
    "to achieve more.\n",
    "\"\"\"\n",
    "question = \"What is florence-vl about\"\n",
    "# question = \"What does florence-vl stand for\"\n",
    "\n",
    "prompt = get_prompt(context, question)\n",
    "print(complete(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dfeb00",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c6106bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb48b4e",
   "metadata": {},
   "source": [
    "#### Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3805e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\n",
    "#     \"feline friends say\",\n",
    "#     \"canine companions say\",\n",
    "#     \"meow\",\n",
    "#     \"woof\",\n",
    "# ]\n",
    "# df = pd.DataFrame()\n",
    "# df[\"sentence\"] = sentences\n",
    "# df[\"embedding\"] = df[\"sentence\"].apply(lambda x: openai.Embedding.create(input=x, deployment_id=\"text-similarity-davinci-001\"))\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f90965",
   "metadata": {},
   "source": [
    "#### Text Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a365c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2061: What\n",
      "318:  is\n",
      "781:  fl\n",
      "382: ore\n",
      "1198: nce\n",
      "12: -\n",
      "19279: vl\n",
      "546:  about\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def get_tokens(text):\n",
    "    return encoding.encode(text)\n",
    "\n",
    "tokens = get_tokens(\"What is florence-vl about\")\n",
    "for token in tokens:\n",
    "    print(f\"{token}:\", encoding.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d4f4a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://www.microsoft.com/en-us/research/project/project-florence-vl/\")\n",
    "soup = BeautifulSoup(r.text, features='html.parser')\n",
    "text = soup.get_text()\n",
    "text = re.sub(r\"[\\n\\t]+\", \"\\n\", text)\n",
    "lines = [line for line in text.split(\"\\n\") if len(line) > 30]  # small lines don't have much information\n",
    "text = \"\\n\".join(lines)\n",
    "\n",
    "filepath = \"data/project-florence-vl.txt\"\n",
    "with open(filepath, \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c292dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text):\n",
    "    SIZE = 4000\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), SIZE):  # naive chunking\n",
    "        chunk = text[i:i+SIZE]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "with open(filepath) as f:\n",
    "    text = f.read()\n",
    "chunks = get_chunks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "fc4f8f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>Project Florence-VL - Microsoft Research\\n    ...</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>on model trained on 800M image-text pairs. GIT...</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>he common interface for all pre-training and d...</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>that the more and more accurate pseudo labels ...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file  \\\n",
       "0  data/project-florence-vl.txt   \n",
       "1  data/project-florence-vl.txt   \n",
       "2  data/project-florence-vl.txt   \n",
       "3  data/project-florence-vl.txt   \n",
       "\n",
       "                                          chunk_text  chunk_token_count  \n",
       "0  Project Florence-VL - Microsoft Research\\n    ...                847  \n",
       "1  on model trained on 800M image-text pairs. GIT...                879  \n",
       "2  he common interface for all pre-training and d...                825  \n",
       "3  that the more and more accurate pseudo labels ...                288  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"file\": [filepath] * len(chunks),\n",
    "    \"chunk_text\": chunks,\n",
    "})\n",
    "df[\"chunk_token_count\"] = df[\"chunk_text\"].apply(lambda x: len(get_tokens(x)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ed26448d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>Project Florence-VL - Microsoft Research\\n    ...</td>\n",
       "      <td>847</td>\n",
       "      <td>[0.0013549693394452333, 0.008274282328784466, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>on model trained on 800M image-text pairs. GIT...</td>\n",
       "      <td>879</td>\n",
       "      <td>[-0.002092509064823389, 0.0050740959122776985,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>he common interface for all pre-training and d...</td>\n",
       "      <td>825</td>\n",
       "      <td>[-0.010482553392648697, 0.006804130505770445, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>that the more and more accurate pseudo labels ...</td>\n",
       "      <td>288</td>\n",
       "      <td>[-0.004963461309671402, 0.007704003248363733, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file  \\\n",
       "0  data/project-florence-vl.txt   \n",
       "1  data/project-florence-vl.txt   \n",
       "2  data/project-florence-vl.txt   \n",
       "3  data/project-florence-vl.txt   \n",
       "\n",
       "                                          chunk_text  chunk_token_count  \\\n",
       "0  Project Florence-VL - Microsoft Research\\n    ...                847   \n",
       "1  on model trained on 800M image-text pairs. GIT...                879   \n",
       "2  he common interface for all pre-training and d...                825   \n",
       "3  that the more and more accurate pseudo labels ...                288   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0013549693394452333, 0.008274282328784466, ...  \n",
       "1  [-0.002092509064823389, 0.0050740959122776985,...  \n",
       "2  [-0.010482553392648697, 0.006804130505770445, ...  \n",
       "3  [-0.004963461309671402, 0.007704003248363733, ...  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(text, model):\n",
    "    response = openai.Embedding.create(input=text, deployment_id=model)\n",
    "    return response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "df[\"embedding\"] = df[\"chunk_text\"].apply(lambda x: get_embedding(x, \"text-search-davinci-doc-001\"))  # token limit: ~2000\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e23dc814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>Project Florence-VL - Microsoft Research\\n    ...</td>\n",
       "      <td>847</td>\n",
       "      <td>[0.0013549693394452333, 0.008274282328784466, ...</td>\n",
       "      <td>0.364001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>on model trained on 800M image-text pairs. GIT...</td>\n",
       "      <td>879</td>\n",
       "      <td>[-0.002092509064823389, 0.0050740959122776985,...</td>\n",
       "      <td>0.233602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>he common interface for all pre-training and d...</td>\n",
       "      <td>825</td>\n",
       "      <td>[-0.010482553392648697, 0.006804130505770445, ...</td>\n",
       "      <td>0.244051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/project-florence-vl.txt</td>\n",
       "      <td>that the more and more accurate pseudo labels ...</td>\n",
       "      <td>288</td>\n",
       "      <td>[-0.004963461309671402, 0.007704003248363733, ...</td>\n",
       "      <td>0.233082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file  \\\n",
       "0  data/project-florence-vl.txt   \n",
       "1  data/project-florence-vl.txt   \n",
       "2  data/project-florence-vl.txt   \n",
       "3  data/project-florence-vl.txt   \n",
       "\n",
       "                                          chunk_text  chunk_token_count  \\\n",
       "0  Project Florence-VL - Microsoft Research\\n    ...                847   \n",
       "1  on model trained on 800M image-text pairs. GIT...                879   \n",
       "2  he common interface for all pre-training and d...                825   \n",
       "3  that the more and more accurate pseudo labels ...                288   \n",
       "\n",
       "                                           embedding  similarity  \n",
       "0  [0.0013549693394452333, 0.008274282328784466, ...    0.364001  \n",
       "1  [-0.002092509064823389, 0.0050740959122776985,...    0.233602  \n",
       "2  [-0.010482553392648697, 0.006804130505770445, ...    0.244051  \n",
       "3  [-0.004963461309671402, 0.007704003248363733, ...    0.233082  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_qa_df(p_df, p_question):\n",
    "    question_embedding = get_embedding(p_question, \"text-search-davinci-query-001\")  # token limit: ~2000\n",
    "    qa_df = p_df.copy()\n",
    "    qa_df[\"similarity\"] = df[\"embedding\"].apply(lambda x: cosine_similarity(x, question_embedding))\n",
    "    return qa_df\n",
    "\n",
    "# question = \"what are multi channel videos\"\n",
    "# question = \"what is the full form of GLIP and what is it about\"\n",
    "question = \"what is the difference between project florence and project florence-vl\"\n",
    "qa_df = get_qa_df(df, question)\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7a0a6a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer0: Project Florence is the overarching project that includes Florence-VL as a subproject. Florence-VL is focused on building new foundation models for Multimodal Intelligence, while the goals of the larger Project Florence are not specified in the text.\n",
      "Answer1: I don't know. \n",
      "Answer2: I don't know.\n",
      "Answer3: I don't know.\n"
     ]
    }
   ],
   "source": [
    "def get_answer(context, question):\n",
    "    prompt = get_prompt(context, question)\n",
    "    return complete(prompt)\n",
    "\n",
    "answers = list(qa_df[\"chunk_text\"].apply(lambda x: get_answer(x, question)))\n",
    "for i, answer in enumerate(answers):\n",
    "    print(f\"Answer{i}:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9b1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-demo-tprompt",
   "language": "python",
   "name": "gpt-demo-tprompt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
